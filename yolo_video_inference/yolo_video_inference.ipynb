{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKA_7ncjyK9M",
        "outputId": "39942cde-4685-4a72-d052-a78c7f635a4f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the model and video\n",
        "# video:\n",
        "# https://www.pexels.com/video/people-using-escalators-in-the-mall-1338592/\n",
        "\n",
        "# v4 tiny weights:\n",
        "# https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.weights\n",
        "# v4 tiny config:\n",
        "# https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4-tiny.cfg7\n",
        "\n",
        "# v3 weights:\n",
        "# https://pjreddie.com/media/files/yolov3.weights\n",
        "# v3 config:\n",
        "# https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg\n",
        "\n",
        "# labels:\n",
        "# https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names\n",
        "# and place it to appropriate location"
      ],
      "metadata": {
        "id": "wbMxO7gUztN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a path to folder with the video and yolo files\n",
        "path = 'drive/MyDrive/Colab_Notebooks_2/yolo_video_inference' "
      ],
      "metadata": {
        "id": "HLnrePWW2hD_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detecting Objects on Image with OpenCV deep learning library\n",
        "#\n",
        "# How does YOLO-v3 Algorithm works for this example case:\n",
        "# STEP1: Reading input video\n",
        "# STEP2: Loading YOLO v3 Network\n",
        "# STEP3: Reading frames in the loop\n",
        "# STEP4: Getting blob from the frame\n",
        "# STEP4: Implementing Forward Pass\n",
        "# STEP5: Getting Bounding Boxes\n",
        "# STEP6: Non-maximum Suppression\n",
        "# STEP7: Drawing Bounding Boxes with Labels\n",
        "# STEP8: creating a new video by writing processed frames\n",
        "#\n",
        "# Result:\n",
        "# New video file with Detected Objects, Bounding Boxes and Labels\n",
        "\n",
        "\n",
        "# Importing needed libraries\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import cv2\n",
        "import time\n",
        "print (cv2.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRWyk60n2PGC",
        "outputId": "67fe6bed-054a-4791-c091-b496da2aaf3c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "==================     STEP1   ===================\n",
        "Start of: Reading input video\n",
        "\"\"\"\n",
        "#NOTE:\n",
        "# Defining 'VideoCapture' object and reading video from a file make sure that the path and file name is correct\n",
        "video = cv2.VideoCapture(os.path.join(path, 'pexels-richarles-moral-1338592-1920x1080-30fps.mp4'))\n",
        "\n",
        "# Preparing variable for writer that we will use to write processed frames\n",
        "writer = None\n",
        "\n",
        "# Preparing variables for spatial dimensions of the frames\n",
        "h, w = None, None\n",
        "\n",
        "\"\"\"\n",
        "End of:\n",
        "Reading input video\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YYxTxHwv2QYp",
        "outputId": "ad25bde6-b858-4268-89a4-67ed1681c321"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEnd of:\\nReading input video\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####### YOLO V4 TINY #######\n",
        "\n",
        "\"\"\"\n",
        "==================     STEP2  ===================\n",
        "Start of: Loading YOLO v4 network\n",
        "\"\"\"\n",
        "\n",
        "# Loading COCO class labels from file and Opening file\n",
        "with open(os.path.join(path, 'coco.names')) as f:\n",
        "    # Getting labels reading every line\n",
        "    # and putting them into the list\n",
        "    labels = [line.strip() for line in f]\n",
        "\n",
        "\n",
        "# # Check point\n",
        "# print('List with labels names:')\n",
        "# print(labels)\n",
        "\n",
        "# Loading trained YOLO v3 Objects Detector with the help of 'dnn' library from OpenCV\n",
        "network = cv2.dnn.readNetFromDarknet(os.path.join(path, 'yolov4-tiny.cfg'),\n",
        "                                     os.path.join(path, 'yolov4-tiny.weights'))\n",
        "\n",
        "# Getting list with names of all layers from YOLO v3 network\n",
        "layers_names_all = network.getLayerNames()\n",
        "\n",
        "# # Check point\n",
        "# print()\n",
        "# print(layers_names_all)\n",
        "\n",
        "# Getting only output layers' names that we need from YOLO v3 algorithm\n",
        "# with function that returns indexes of layers with unconnected outputs\n",
        "#layers_names_output = [layers_names_all[i[0] - 1] for i in network.getUnconnectedOutLayers()] # change needed for yolov4tiny\n",
        "layers_names_output = [layers_names_all[i - 1] for i in network.getUnconnectedOutLayers()]\n",
        "\n",
        "\n",
        "# # Check point\n",
        "# print()\n",
        "# print(layers_names_output)  # ['yolo_82', 'yolo_94', 'yolo_106']\n",
        "\n",
        "# Setting minimum probability to eliminate weak predictions\n",
        "probability_minimum = 0.5\n",
        "\n",
        "# Setting threshold for filtering weak bounding boxes\n",
        "# with non-maximum suppression\n",
        "threshold = 0.3\n",
        "\n",
        "# Generating colours for representing every detected object\n",
        "# with function randint(low, high=None, size=None, dtype='l')\n",
        "colours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n",
        "\n",
        "# # Check point\n",
        "# print(colours.shape)\n",
        "# print(colours[0])\n",
        "\n",
        "\"\"\"\n",
        "End of:\n",
        "Loading YOLO v3 network\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "a_XEMx1j2c6h",
        "outputId": "069aa456-84ef-4785-8a55-ae4b0472a2b9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEnd of:\\nLoading YOLO v3 network\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####### YOLO V3 #######\n",
        "\n",
        "\"\"\"\n",
        "==================     STEP2  ===================\n",
        "Start of: Loading YOLO v3 network\n",
        "\"\"\"\n",
        "\n",
        "# Loading COCO class labels from file and Opening file\n",
        "with open(os.path.join(path, 'coco.names')) as f:\n",
        "    # Getting labels reading every line\n",
        "    # and putting them into the list\n",
        "    labels = [line.strip() for line in f]\n",
        "\n",
        "\n",
        "# # Check point\n",
        "# print('List with labels names:')\n",
        "# print(labels)\n",
        "\n",
        "# Loading trained YOLO v3 Objects Detector with the help of 'dnn' library from OpenCV\n",
        "network = cv2.dnn.readNetFromDarknet(os.path.join(path, 'yolov3.cfg'),\n",
        "                                     os.path.join(path, 'yolov3.weights'))\n",
        "\n",
        "# Getting list with names of all layers from YOLO v3 network\n",
        "layers_names_all = network.getLayerNames()\n",
        "\n",
        "# # Check point\n",
        "# print()\n",
        "# print(layers_names_all)\n",
        "\n",
        "# Getting only output layers' names that we need from YOLO v3 algorithm\n",
        "# with function that returns indexes of layers with unconnected outputs\n",
        "#layers_names_output = [layers_names_all[i[0] - 1] for i in network.getUnconnectedOutLayers()] # change needed for yolov4tiny\n",
        "layers_names_output = [layers_names_all[i - 1] for i in network.getUnconnectedOutLayers()]\n",
        "\n",
        "\n",
        "# # Check point\n",
        "# print()\n",
        "#print(layers_names_output)  # ['yolo_82', 'yolo_94', 'yolo_106']\n",
        "\n",
        "# Setting minimum probability to eliminate weak predictions\n",
        "probability_minimum = 0.5\n",
        "\n",
        "# Setting threshold for filtering weak bounding boxes\n",
        "# with non-maximum suppression\n",
        "threshold = 0.3\n",
        "\n",
        "# Generating colours for representing every detected object\n",
        "# with function randint(low, high=None, size=None, dtype='l')\n",
        "colours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n",
        "\n",
        "# # Check point\n",
        "# print(colours.shape)\n",
        "# print(colours[0])\n",
        "\n",
        "\"\"\"\n",
        "End of:\n",
        "Loading YOLO v3 network\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kaoKEFx48ejY",
        "outputId": "1d383126-30cf-403c-e271-1707a72b305b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEnd of:\\nLoading YOLO v3 network\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BbV-QOOixPbY",
        "outputId": "2d7c387d-b353-4118-a244-ba8b0d04b044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame number 1 took 3.11579 seconds\n",
            "Frame number 2 took 0.87590 seconds\n",
            "Frame number 3 took 0.86650 seconds\n",
            "Frame number 4 took 0.85117 seconds\n",
            "Frame number 5 took 0.82223 seconds\n",
            "Frame number 6 took 0.87551 seconds\n",
            "Frame number 7 took 1.30070 seconds\n",
            "Frame number 8 took 1.30462 seconds\n",
            "Frame number 9 took 1.24638 seconds\n",
            "Frame number 10 took 0.83051 seconds\n",
            "Frame number 11 took 1.08565 seconds\n",
            "Frame number 12 took 0.93969 seconds\n",
            "Frame number 13 took 0.85007 seconds\n",
            "Frame number 14 took 0.84890 seconds\n",
            "Frame number 15 took 0.87037 seconds\n",
            "Frame number 16 took 0.87103 seconds\n",
            "Frame number 17 took 0.83870 seconds\n",
            "Frame number 18 took 0.91306 seconds\n",
            "Frame number 19 took 0.84403 seconds\n",
            "Frame number 20 took 1.23020 seconds\n",
            "Frame number 21 took 1.25195 seconds\n",
            "Frame number 22 took 1.23541 seconds\n",
            "Frame number 23 took 0.88265 seconds\n",
            "Frame number 24 took 0.84884 seconds\n",
            "Frame number 25 took 0.82547 seconds\n",
            "Frame number 26 took 0.82790 seconds\n",
            "Frame number 27 took 0.84285 seconds\n",
            "Frame number 28 took 0.81147 seconds\n",
            "Frame number 29 took 0.84514 seconds\n",
            "Frame number 30 took 0.84411 seconds\n",
            "Frame number 31 took 0.83967 seconds\n",
            "Frame number 32 took 0.81791 seconds\n",
            "Frame number 33 took 0.83780 seconds\n",
            "Frame number 34 took 1.18894 seconds\n",
            "Frame number 35 took 1.27106 seconds\n",
            "Frame number 36 took 1.24067 seconds\n",
            "Frame number 37 took 0.84174 seconds\n",
            "Frame number 38 took 0.83043 seconds\n",
            "Frame number 39 took 0.82096 seconds\n",
            "Frame number 40 took 0.83045 seconds\n",
            "Frame number 41 took 0.85194 seconds\n",
            "Frame number 42 took 0.80708 seconds\n",
            "Frame number 43 took 0.80203 seconds\n",
            "Frame number 44 took 0.82540 seconds\n",
            "Frame number 45 took 0.82476 seconds\n",
            "Frame number 46 took 0.81731 seconds\n",
            "Frame number 47 took 0.81631 seconds\n",
            "Frame number 48 took 1.15265 seconds\n",
            "Frame number 49 took 1.27720 seconds\n",
            "Frame number 50 took 1.23956 seconds\n",
            "Frame number 51 took 0.93100 seconds\n",
            "Frame number 52 took 0.83612 seconds\n",
            "Frame number 53 took 0.82831 seconds\n",
            "Frame number 54 took 0.79934 seconds\n",
            "Frame number 55 took 0.81996 seconds\n",
            "Frame number 56 took 0.84985 seconds\n",
            "Frame number 57 took 0.83020 seconds\n",
            "Frame number 58 took 0.95694 seconds\n",
            "Frame number 59 took 0.91000 seconds\n",
            "Frame number 60 took 0.83848 seconds\n",
            "Frame number 61 took 0.85547 seconds\n",
            "Frame number 62 took 1.25819 seconds\n",
            "Frame number 63 took 1.25212 seconds\n",
            "Frame number 64 took 1.22835 seconds\n",
            "Frame number 65 took 0.85141 seconds\n",
            "Frame number 66 took 0.84197 seconds\n",
            "Frame number 67 took 0.82367 seconds\n",
            "Frame number 68 took 0.82325 seconds\n",
            "Frame number 69 took 0.83583 seconds\n",
            "Frame number 70 took 0.82861 seconds\n",
            "Frame number 71 took 0.82508 seconds\n",
            "Frame number 72 took 0.83501 seconds\n",
            "Frame number 73 took 0.85767 seconds\n",
            "Frame number 74 took 0.82689 seconds\n",
            "Frame number 75 took 0.82028 seconds\n",
            "Frame number 76 took 1.23986 seconds\n",
            "Frame number 77 took 1.24458 seconds\n",
            "Frame number 78 took 1.31672 seconds\n",
            "Frame number 79 took 0.81367 seconds\n",
            "Frame number 80 took 0.80947 seconds\n",
            "Frame number 81 took 0.82590 seconds\n",
            "Frame number 82 took 0.82509 seconds\n",
            "Frame number 83 took 0.81925 seconds\n",
            "Frame number 84 took 0.82571 seconds\n",
            "Frame number 85 took 0.82603 seconds\n",
            "Frame number 86 took 0.89407 seconds\n",
            "Frame number 87 took 0.84374 seconds\n",
            "Frame number 88 took 0.83308 seconds\n",
            "Frame number 89 took 0.92265 seconds\n",
            "Frame number 90 took 1.24210 seconds\n",
            "Frame number 91 took 1.22280 seconds\n",
            "Frame number 92 took 1.17167 seconds\n",
            "Frame number 93 took 0.81509 seconds\n",
            "Frame number 94 took 0.80982 seconds\n",
            "Frame number 95 took 0.83023 seconds\n",
            "Frame number 96 took 0.85823 seconds\n",
            "Frame number 97 took 0.83926 seconds\n",
            "Frame number 98 took 0.84796 seconds\n",
            "Frame number 99 took 0.82592 seconds\n",
            "Frame number 100 took 0.82841 seconds\n",
            "Frame number 101 took 0.83308 seconds\n",
            "Frame number 102 took 0.83475 seconds\n",
            "Frame number 103 took 0.84695 seconds\n",
            "Frame number 104 took 1.25578 seconds\n",
            "Frame number 105 took 1.26902 seconds\n",
            "Frame number 106 took 1.25333 seconds\n",
            "Frame number 107 took 0.84551 seconds\n",
            "Frame number 108 took 0.89365 seconds\n",
            "Frame number 109 took 0.88963 seconds\n",
            "Frame number 110 took 0.82240 seconds\n",
            "Frame number 111 took 0.83964 seconds\n",
            "Frame number 112 took 0.84031 seconds\n",
            "Frame number 113 took 0.83818 seconds\n",
            "Frame number 114 took 0.83086 seconds\n",
            "Frame number 115 took 0.82612 seconds\n",
            "Frame number 116 took 0.85679 seconds\n",
            "Frame number 117 took 0.90401 seconds\n",
            "Frame number 118 took 1.25050 seconds\n",
            "Frame number 119 took 1.29078 seconds\n",
            "Frame number 120 took 1.19851 seconds\n",
            "Frame number 121 took 0.85189 seconds\n",
            "Frame number 122 took 0.82937 seconds\n",
            "Frame number 123 took 0.82537 seconds\n",
            "Frame number 124 took 0.84055 seconds\n",
            "Frame number 125 took 0.83759 seconds\n",
            "Frame number 126 took 0.82426 seconds\n",
            "Frame number 127 took 0.82592 seconds\n",
            "Frame number 128 took 0.84258 seconds\n",
            "Frame number 129 took 0.85865 seconds\n",
            "Frame number 130 took 0.81514 seconds\n",
            "Frame number 131 took 0.89819 seconds\n",
            "Frame number 132 took 1.20199 seconds\n",
            "Frame number 133 took 1.23511 seconds\n",
            "Frame number 134 took 1.14843 seconds\n",
            "Frame number 135 took 0.81430 seconds\n",
            "Frame number 136 took 0.83178 seconds\n",
            "Frame number 137 took 0.82803 seconds\n",
            "Frame number 138 took 0.84258 seconds\n",
            "Frame number 139 took 0.88542 seconds\n",
            "Frame number 140 took 0.86245 seconds\n",
            "Frame number 141 took 0.83547 seconds\n",
            "Frame number 142 took 0.83435 seconds\n",
            "Frame number 143 took 0.84110 seconds\n",
            "Frame number 144 took 0.82992 seconds\n",
            "Frame number 145 took 0.89420 seconds\n",
            "Frame number 146 took 1.28559 seconds\n",
            "Frame number 147 took 1.22481 seconds\n",
            "Frame number 148 took 1.15017 seconds\n",
            "Frame number 149 took 0.84329 seconds\n",
            "Frame number 150 took 0.84882 seconds\n",
            "Frame number 151 took 0.83268 seconds\n",
            "Frame number 152 took 0.81322 seconds\n",
            "Frame number 153 took 0.81973 seconds\n",
            "Frame number 154 took 0.84641 seconds\n",
            "Frame number 155 took 0.82118 seconds\n",
            "Frame number 156 took 0.82264 seconds\n",
            "Frame number 157 took 0.83117 seconds\n",
            "Frame number 158 took 0.82797 seconds\n",
            "Frame number 159 took 0.90325 seconds\n",
            "Frame number 160 took 1.24953 seconds\n",
            "Frame number 161 took 1.21751 seconds\n",
            "Frame number 162 took 1.18501 seconds\n",
            "Frame number 163 took 0.81790 seconds\n",
            "Frame number 164 took 0.80284 seconds\n",
            "Frame number 165 took 0.80410 seconds\n",
            "Frame number 166 took 0.79134 seconds\n",
            "\n",
            "Total number of frames 166\n",
            "Total amount of time 156.51957 seconds\n",
            "FPS: 1.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nSome comments\\n\\nWhat is a FOURCC?\\n    FOURCC is short for \"four character code\" - an identifier for a video codec,\\n    compression format, colour or pixel format used in media files.\\n    http://www.fourcc.org\\n\\n\\nParameters for cv2.VideoWriter():\\n    filename - Name of the output video file.\\n    fourcc - 4-character code of codec used to compress the frames.\\n    fps\\t- Frame rate of the created video.\\n    frameSize - Size of the video frames.\\n    isColor\\t- If it True, the encoder will expect and encode colour frames.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "==================     STEP3  ===================\n",
        "Start of: Reading frames in the loop\n",
        "\"\"\"\n",
        "\n",
        "# Defining variable for counting frames at the end we will show total amount of processed frames\n",
        "f = 0\n",
        "\n",
        "# Defining variable for counting total time At the end we will show time spent for processing all frames\n",
        "t = 0\n",
        "\n",
        "# Defining loop for catching frames \n",
        "while True:\n",
        "    # Capturing frame-by-frame\n",
        "    ret, frame = video.read()\n",
        "\n",
        "    # If the frame was not retrieved e.g.: at the end of the video, then we break the loop\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Getting spatial dimensions of the frame as we do it only once from the very beginning\n",
        "    # all other frames have the same dimension\n",
        "    if w is None or h is None:\n",
        "        # Slicing from tuple only first two elements\n",
        "        h, w = frame.shape[:2]\n",
        "    \n",
        "    \"\"\"\n",
        "    End of: Reading frame in loop\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    ==================     STEP4  ===================\n",
        "    Start of: Getting blob from current frame\n",
        "    \"\"\"\n",
        "\n",
        "    # Getting blob from current frame\n",
        "    # The 'cv2.dnn.blobFromImage' function returns 4-dimensional blob from current\n",
        "    # frame after mean subtraction, normalizing, and RB channels swapping\n",
        "    # Resulted shape has number of frames, number of channels, width and height\n",
        "    # e.g.:\n",
        "    # blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size, mean, swapRB=True)\n",
        "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),\n",
        "                                 swapRB=True, crop=False)\n",
        "\n",
        "    \"\"\"\n",
        "    End of: Getting blob from current frame\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    ==================     STEP4  ===================\n",
        "    Start of: Implementing Forward pass\n",
        "    \"\"\"\n",
        "\n",
        "    # Implementing forward pass with our blob and only through output layers\n",
        "    # Calculating at the same time, needed time for forward pass\n",
        "    network.setInput(blob)  # setting blob as input to the network\n",
        "    start = time.time()\n",
        "    output_from_network = network.forward(layers_names_output)\n",
        "    end = time.time()\n",
        "\n",
        "    # Increasing counters for frames and total time\n",
        "    f += 1\n",
        "    t += end - start\n",
        "\n",
        "    # Showing spent time for single current frame\n",
        "    print('Frame number {0} took {1:.5f} seconds'.format(f, end - start))\n",
        "    \n",
        "    \"\"\"\n",
        "    End of:Implementing Forward pass\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    ==================     STEP5  ===================\n",
        "    Start of: Getting bounding boxes\n",
        "    \"\"\"\n",
        "\n",
        "    # Preparing lists for detected bounding boxes, obtained confidences and class's number\n",
        "    bounding_boxes = []\n",
        "    confidences = []\n",
        "    classIDs = []\n",
        "\n",
        "    # Going through all output layers after feed forward pass\n",
        "    for result in output_from_network:\n",
        "        # Going through all detections from current output layer\n",
        "        for detected_objects in result:\n",
        "            # Getting 80 classes' probabilities for current detected object\n",
        "            scores = detected_objects[5:]\n",
        "            # Getting index of the class with the maximum value of probability\n",
        "            class_current = np.argmax(scores)\n",
        "            # Getting value of probability for defined class\n",
        "            confidence_current = scores[class_current]\n",
        "            \n",
        "            # # Every 'detected_objects' numpy array has first 4 numbers with\n",
        "            # # bounding box coordinates and rest 80 with probabilities\n",
        "            #  # for every class\n",
        "            # print(detected_objects.shape)  # (85,)\n",
        "\n",
        "            # Eliminating weak predictions with minimum probability\n",
        "            if confidence_current > probability_minimum:\n",
        "                # Scaling bounding box coordinates to the initial frame size\n",
        "                # YOLO data format keeps coordinates for center of bounding box\n",
        "                # and its current width and height\n",
        "                # That is why we can just multiply them elementwise\n",
        "                # to the width and height\n",
        "                # of the original frame and in this way get coordinates for center\n",
        "                # of bounding box, its width and height for original frame\n",
        "                box_current = detected_objects[0:4] * np.array([w, h, w, h])\n",
        "\n",
        "                # Now, from YOLO data format, we can get top left corner coordinates that are x_min and y_min\n",
        "                x_center, y_center, box_width, box_height = box_current\n",
        "                x_min = int(x_center - (box_width / 2))\n",
        "                y_min = int(y_center - (box_height / 2))\n",
        "\n",
        "                # Adding results into prepared lists\n",
        "                bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n",
        "                confidences.append(float(confidence_current))\n",
        "                classIDs.append(class_current)\n",
        "            \t\n",
        "        \"\"\"\t\n",
        "        End of: Getting bounding boxes\n",
        "        \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    ==================     STEP6   ===================\n",
        "    Start of: Non-maximum suppression\n",
        "    \"\"\"\n",
        "\n",
        "    # Implementing non-maximum suppression of given bounding boxes\n",
        "    # With this technique we exclude some of bounding boxes if their\n",
        "    # corresponding confidences are low or there is another\n",
        "    # bounding box for this region with higher confidence\n",
        "\n",
        "    # It is needed to make sure that data type of the boxes is 'int'\n",
        "    # and data type of the confidences is 'float'\n",
        "    # https://github.com/opencv/opencv/issues/12789\n",
        "    results = cv2.dnn.NMSBoxes(bounding_boxes, confidences,\n",
        "                               probability_minimum, threshold)\n",
        "    \n",
        "    \"\"\"\n",
        "    End of: Non-maximum suppression\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    ==================     STEP6   ===================\n",
        "    Start of: Drawing bounding boxes and labels\n",
        "    \"\"\"\n",
        "\n",
        "    # Checking if there is at least one detected object after non-maximum suppression\n",
        "    if len(results) > 0:\n",
        "        # Going through indexes of results\n",
        "        for i in results.flatten():\n",
        "            # Getting current bounding box coordinates, its width and height\n",
        "            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n",
        "            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n",
        "\n",
        "            # Preparing colour for current bounding box and converting from numpy array to list\n",
        "            colour_box_current = colours[classIDs[i]].tolist()\n",
        "\n",
        "            # print(type(colour_box_current))  # <class 'list'>\n",
        "            # print(colour_box_current)  # [172 , 10, 127]\n",
        "\n",
        "            # Drawing bounding box on the original current frame\n",
        "            cv2.rectangle(frame, (x_min, y_min),\n",
        "                          (x_min + box_width, y_min + box_height),\n",
        "                          colour_box_current, 2)\n",
        "\n",
        "            # Preparing text with label and confidence for current bounding box\n",
        "            text_box_current = '{}: {:.4f}'.format(labels[int(classIDs[i])],\n",
        "                                                   confidences[i])\n",
        "\n",
        "            # Putting text with label and confidence on the original image\n",
        "            cv2.putText(frame, text_box_current, (x_min, y_min - 5),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)\n",
        "\n",
        "    \"\"\"\n",
        "    End of:\n",
        "    Drawing bounding boxes and labels\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    ==================     STEP7   ===================\n",
        "    Start of: Writing processed frame into the file\n",
        "    \"\"\"\n",
        "\n",
        "    # Initializing writer\n",
        "    # we do it only once from the very beginning when we get spatial dimensions of the frames\n",
        "    if writer is None:\n",
        "        # Constructing code of the codec to be used in the function VideoWriter\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "\n",
        "        # Writing current processed frame into the video file\n",
        "        writer = cv2.VideoWriter(os.path.join(path, 'video_result.mp4'), fourcc, 30,\n",
        "                                 (frame.shape[1], frame.shape[0]), True)\n",
        "\n",
        "    # Write processed current frame to the file\n",
        "    writer.write(frame)\n",
        "\n",
        "\"\"\"\n",
        "End of: Writing processed frame into the file\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "End of: Writing processed frame into the file\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Printing final results\n",
        "print()\n",
        "print('Total number of frames', f)\n",
        "print('Total amount of time {:.5f} seconds'.format(t))\n",
        "print('FPS:', round((f / t), 1))\n",
        "\n",
        "\n",
        "# Releasing video reader and writer\n",
        "video.release()\n",
        "writer.release()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Some comments\n",
        "\n",
        "What is a FOURCC?\n",
        "    FOURCC is short for \"four character code\" - an identifier for a video codec,\n",
        "    compression format, colour or pixel format used in media files.\n",
        "    http://www.fourcc.org\n",
        "\n",
        "\n",
        "Parameters for cv2.VideoWriter():\n",
        "    filename - Name of the output video file.\n",
        "    fourcc - 4-character code of codec used to compress the frames.\n",
        "    fps\t- Frame rate of the created video.\n",
        "    frameSize - Size of the video frames.\n",
        "    isColor\t- If it True, the encoder will expect and encode colour frames.\n",
        "\"\"\""
      ]
    }
  ]
}